{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674eb5f2-027a-41d1-9933-c552d79d83e8",
   "metadata": {},
   "source": [
    "# **BERTopic on Italian, Spanish, German and English on Amazon_massive_intent dataset**\n",
    "In our earlier experiment with English news articles, the KeyBERT-inspired topic representations clearly outperformed basic frequency-based labels (c-TF-IDF), giving us cleaner and more meaningful descriptions.\n",
    "\n",
    "We then tested the same approach on German and French Amazon reviews, which are much messier due to slang, typos, and strong opinions. We trained separate BERTopic models for each language and applied the KeyBERT-style method to see if we could still get clear product-related topics.However, the results were much weaker: both languages reached coherence scores of only 0.10–0.14, far below the ~0.40 we achieved on the BBC News dataset.\n",
    "\n",
    "**In this notebook,** we test whether the same strategy works in a multilingual setting using the **amazon_massive_intent dataset** in Italian, Spanish, German and English. This dataset is simpler and cleaner than Amazon Reviews, so we want to see if the model performs better when there is less linguistic noise.\n",
    "\n",
    "We train separate BERTopic models for each language, just like we did with the Amazon Reviews experiments, and apply the KeyBERT-inspired representation to check whether we can extract clear and interpretable for **assistant command patterns**.\n",
    "\n",
    "\n",
    "We used the English multilingual dataset to check whether the model performs better in English, since English data and embeddings are generally higher quality, making it a good reference point for comparison.Also, we included the German of this dataset to compare how the same language performs on a different less noisy dataset.\n",
    "\n",
    "\n",
    "**Goal:** To assess whether BERTopic can consistently uncover similar **intent-related clusters** across three different languages Italian, German, English in the massive dataset, and to analyze how language-specific phrasing influences cluster structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9229f3b8-8e01-4969-a2cc-edeb4c1327a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.13/site-packages (25.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.1\n",
      "    Uninstalling pip-25.1:\n",
      "      Successfully uninstalled pip-25.1\n",
      "Successfully installed pip-25.3\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.17.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp313-cp313-macosx_10_13_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.6.1)\n",
      "Collecting hdbscan>=0.8.29 (from bertopic)\n",
      "  Downloading hdbscan-0.8.40.tar.gz (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "  Installingdone\n",
      "\u001b[?25h  Getting requirements to build wheel ...done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting umap-learn>=0.5.0 (from bertopic)\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.13/site-packages (from bertopic) (2.1.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/anaconda3/lib/python3.13/site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/anaconda3/lib/python3.13/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: llvmlite>0.36.0 in /opt/anaconda3/lib/python3.13/site-packages (from bertopic) (0.44.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (3.17.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.2)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: shellingham in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.0)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading sentence_transformers-4.0.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-2.2.1.tar.gz (84 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?2done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tokenizers>=0.10.3 (from sentence_transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.0.0.tar.gz (85 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.2.1.tar.gz (80 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.2.0.tar.gz (81 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.1.1.tar.gz (81 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.1.0.tar.gz (78 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Downloading sentence-transformers-1.0.4.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.0.3.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.0.2.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.0.1.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-1.0.0.tar.gz (74 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.4.1.2.tar.gz (64 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.4.1.1.tar.gz (64 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?2done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.4.1.tar.gz (64 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bertopic\n",
      "  Downloading bertopic-0.17.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.4.0.tar.gz (65 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.9.tar.gz (64 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25hCollecting transformers<3.6.0,>=3.1.0 (from sentence_transformers)\n",
      "  Downloading transformers-3.5.1-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<3.4.0,>=3.1.0 (from sentence_transformers)\n",
      "  Downloading transformers-3.3.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.3.7.2.tar.gz (59 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.7.1.tar.gz (59 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.7.tar.gz (59 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.6.tar.gz (62 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<3.2.0,>=3.1.0 (from sentence_transformers)\n",
      "  Downloading transformers-3.1.0-py3-none-any.whl.metadata (49 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.3.5.1.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==3.0.2 (from sentence_transformers)\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.3.5.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.4.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.3.tar.gz (65 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.2.tar.gz (65 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.1.tar.gz (64 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.3.0.tar.gz (61 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.2.6.2.tar.gz (60 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==2.11.0 (from sentence_transformers)\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl.metadata (45 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.2.6.1.tar.gz (55 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.2.5.1.tar.gz (52 kB)\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==2.3.0 (from sentence_transformers)\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.2.5.tar.gz (49 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.2.4.1.tar.gz (49 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers==2.2.1 (from sentence_transformers)\n",
      "  Downloading transformers-2.2.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.2.4.tar.gz (49 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.2.3.tar.gz (45 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-transformers==1.1.0 (from sentence_transformers)\n",
      "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.2.2.tar.gz (44 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.2.1.tar.gz (42 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-transformers==1.0.0 (from sentence_transformers)\n",
      "  Downloading pytorch_transformers-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-0.2.0.tar.gz (28 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading sentence-transformers-0.1.0.tar.gz (35 kB)\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fsspec[http]<=2025.10.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "\u001b[31mERROR: Cannot install sentence-transformers==0.1.0, sentence-transformers==0.2.0, sentence-transformers==0.2.1, sentence-transformers==0.2.2, sentence-transformers==0.2.3, sentence-transformers==0.2.4, sentence-transformers==0.2.4.1, sentence-transformers==0.2.5, sentence-transformers==0.2.5.1, sentence-transformers==0.2.6.1, sentence-transformers==0.2.6.2, sentence-transformers==0.3.0, sentence-transformers==0.3.1, sentence-transformers==0.3.2, sentence-transformers==0.3.3, sentence-transformers==0.3.4, sentence-transformers==0.3.5, sentence-transformers==0.3.5.1, sentence-transformers==0.3.6, sentence-transformers==0.3.7, sentence-transformers==0.3.7.1, sentence-transformers==0.3.7.2, sentence-transformers==0.3.8, sentence-transformers==0.3.9, sentence-transformers==0.4.0, sentence-transformers==0.4.1, sentence-transformers==0.4.1.1, sentence-transformers==0.4.1.2, sentence-transformers==1.0.0, sentence-transformers==1.0.1, sentence-transformers==1.0.2, sentence-transformers==1.0.3, sentence-transformers==1.0.4, sentence-transformers==1.1.0, sentence-transformers==1.1.1, sentence-transformers==1.2.0, sentence-transformers==1.2.1, sentence-transformers==2.0.0, sentence-transformers==2.1.0, sentence-transformers==2.2.0, sentence-transformers==2.2.1, sentence-transformers==2.2.2, sentence-transformers==2.3.0, sentence-transformers==2.3.1, sentence-transformers==2.4.0, sentence-transformers==2.5.0, sentence-transformers==2.5.1, sentence-transformers==2.6.0, sentence-transformers==2.6.1, sentence-transformers==2.7.0, sentence-transformers==3.0.0, sentence-transformers==3.0.1, sentence-transformers==3.1.0, sentence-transformers==3.1.1, sentence-transformers==3.2.0, sentence-transformers==3.2.1, sentence-transformers==3.3.0, sentence-transformers==3.3.1, sentence-transformers==3.4.0, sentence-transformers==3.4.1, sentence-transformers==4.0.0, sentence-transformers==4.0.1, sentence-transformers==4.0.2, sentence-transformers==4.1.0, sentence-transformers==5.0.0, sentence-transformers==5.1.0, sentence-transformers==5.1.1 and sentence-transformers==5.1.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    sentence-transformers 5.1.2 depends on torch>=1.11.0\n",
      "    sentence-transformers 5.1.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 5.1.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 5.0.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 4.1.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 4.0.2 depends on torch>=1.11.0\n",
      "    sentence-transformers 4.0.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 4.0.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.4.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.4.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.3.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.3.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.2.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.2.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.1.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.1.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.0.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 3.0.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.7.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.6.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.6.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.5.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.5.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.4.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.3.1 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.3.0 depends on torch>=1.11.0\n",
      "    sentence-transformers 2.2.2 depends on torch>=1.6.0\n",
      "    sentence-transformers 2.2.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 2.2.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 2.1.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 2.0.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.2.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.2.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.1.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.1.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.0.4 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.0.3 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.0.2 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.0.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 1.0.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.4.1.2 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.4.1.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.4.1 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.4.0 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.3.9 depends on torch>=1.6.0\n",
      "    sentence-transformers 0.3.8 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.7.2 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.7.1 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.7 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.6 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.5.1 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.5 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.4 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.3 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.2 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.1 depends on torch>=1.2.0\n",
      "    sentence-transformers 0.3.0 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.6.2 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.6.1 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.5.1 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.5 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.4.1 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.4 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.3 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.2 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.1 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.2.0 depends on torch>=1.0.1\n",
      "    sentence-transformers 0.1.0 depends on torch>=1.0.1\n",
      "\n",
      "Additionally, some packages in these conflicts have no matching distributions available for your environment:\n",
      "    torch\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "/opt/anaconda3/bin/python: No module named spacy\n",
      "/opt/anaconda3/bin/python: No module named spacy\n",
      "/opt/anaconda3/bin/python: No module named spacy\n",
      "/opt/anaconda3/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install bertopic datasets sentence_transformers pandas spacy scikit-learn\n",
    "!python -m spacy download de_core_news_sm\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download it_core_news_sm\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329cbf8d-8995-4fee-a157-e4f189931d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_italian = load_dataset(\"SetFit/amazon_massive_intent_it-IT\")\n",
    "docs_train_italian = ds_italian[\"train\"][\"text\"]\n",
    "categories_train_italian = ds_italian[\"train\"][\"label_text\"]\n",
    "\n",
    "\n",
    "ds_de = load_dataset(\"SetFit/amazon_massive_intent_de-DE\")\n",
    "docs_train_de = ds_de[\"train\"][\"text\"]\n",
    "categories_train_de = ds_de[\"train\"][\"label_text\"]\n",
    "\n",
    "ds_english = load_dataset(\"SetFit/amazon_massive_intent_en-US\")\n",
    "docs_train_english = ds_english[\"train\"][\"text\"]\n",
    "categories_train_english = ds_english[\"train\"][\"label_text\"]\n",
    "\n",
    "\n",
    "# spanish dataset\n",
    "ds_es = load_dataset(\"SetFit/amazon_massive_intent_es-ES\")\n",
    "docs_train_es = ds_es[\"train\"][\"text\"]\n",
    "categories_train_es = ds_es[\"train\"][\"label_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37e90b-8ffc-4ec7-b6b2-e4ec8bb62d4a",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3eb28-d0ac-455a-8e07-7fc19c405188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian Dataset\n",
    "import spacy\n",
    "\n",
    "nlp_it = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "\n",
    "def lemmatize_it(text: str):\n",
    "    doc = nlp_it(text)\n",
    "    return [tok.lemma_ for tok in doc if tok.is_alpha]\n",
    "\n",
    "\n",
    "lemmatized_it = [lemmatize_it(text) for text in docs_train_italian[:50000]]\n",
    "\n",
    "#example\n",
    "print(\"Original Italian:\", docs_train_italian[38])\n",
    "print(\"Lemmatized Italian:\", lemmatized_it[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064cb316-4fa5-480a-a665-c4f6926a3253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English Dataset\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def lemmatize_en(text: str):\n",
    "    doc = nlp_en(text)\n",
    "    return [tok.lemma_ for tok in doc if tok.is_alpha]\n",
    "\n",
    "lemmatized_en = [lemmatize_en(text) for text in docs_train_english[:50000]]\n",
    "\n",
    "\n",
    "print(\"Original English:\", docs_train_english[0])\n",
    "print(\"Lemmatized English:\", lemmatized_en[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ce509-07a1-4128-a08d-cc1096814164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# German Dataset\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "def lemmatize_de(text: str):\n",
    "    doc = nlp_de(text)\n",
    "    return [tok.lemma_ for tok in doc if tok.is_alpha]\n",
    "\n",
    "\n",
    "lemmatized_de = [lemmatize_de(text) for text in docs_train_de[:50000]]\n",
    "\n",
    "\n",
    "print(\"Original German:\", docs_train_de[0])\n",
    "print(\"Lemmatized German:\", lemmatized_de[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54928d-10f7-4748-82c9-c7328c7ba262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spanish Dataset\n",
    "import spacy\n",
    "\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "#lemmatization for Spanish only\n",
    "def lemmatize_es(text: str):\n",
    "    doc = nlp_es(text)\n",
    "    return [tok.lemma_ for tok in doc if tok.is_alpha]\n",
    "\n",
    "\n",
    "lemmatized_es = [lemmatize_es(text) for text in docs_train_es[:50000]]\n",
    "\n",
    "#example\n",
    "print(\"Original Spanish:\", docs_train_es[0])\n",
    "print(\"Lemmatized Spanish:\", lemmatized_es[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c1721-e46c-47b4-b64d-315ceac1d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lemmas: list[str], lang: str) -> str:\n",
    "    stopwords_map = {\n",
    "        \"it\": nlp_it.Defaults.stop_words,\n",
    "        \"de\": nlp_de.Defaults.stop_words,\n",
    "        \"en\": nlp_en.Defaults.stop_words,\n",
    "        \"es\": nlp_es.Defaults.stop_words\n",
    "\n",
    "    }\n",
    "\n",
    "    if lang not in stopwords_map:\n",
    "        raise ValueError(\"lang must be 'it' or 'en' or 'es' or 'de' \" )\n",
    "\n",
    "    stopwords = stopwords_map[lang]\n",
    "    cleaned = (lemma for lemma in lemmas if lemma.lower() not in stopwords)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "docs_cleaned_italian = [remove_stopwords(lemmas, \"it\") for lemmas in lemmatized_it]\n",
    "print(\"Cleaned Italian:\", docs_cleaned_italian[38])\n",
    "\n",
    "docs_cleaned_english = [remove_stopwords(lemmas, \"en\") for lemmas in lemmatized_en]\n",
    "print(\"Cleaned English:\", docs_cleaned_english[0])\n",
    "\n",
    "docs_cleaned_de = [remove_stopwords(lemmas, \"de\") for lemmas in lemmatized_de]\n",
    "print(\"Cleaned German:\", docs_cleaned_de[0])\n",
    "\n",
    "docs_cleaned_es = [remove_stopwords(lemmas, \"es\") for lemmas in lemmatized_es]\n",
    "print(\"Cleaned Spanish:\", docs_cleaned_es[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82dc179-d0fc-41ee-a6ab-5ce6aefd4c80",
   "metadata": {},
   "source": [
    "### **Set up**\n",
    "##### **BERTopic model for Italian**\n",
    "\n",
    "We trained a BERTopic model on the Amazon Massive intent dataset using different language embeddings e.g. Italian, German, English. Instead of restricting the model to 10 predefined topics, we allowed BERTopic to learn the topic structure naturally, specifying only a minimum topic size of 30 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63ee6a-0e81-4167-bf3b-f1511fa63ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889b197-b01f-4051-aac5-7f29ff6a7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model_italian = KeyBERTInspired()\n",
    "\n",
    "topic_model_baseline_italian = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"italian\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    min_topic_size=30,\n",
    "    nr_topics=None,\n",
    "    representation_model=representation_model_italian\n",
    ")\n",
    "\n",
    "topics_base_italian, probs_base_italian = topic_model_baseline_italian.fit_transform(docs_train_italian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa8779-c69c-4e25-8218-d943b472d323",
   "metadata": {},
   "source": [
    "##### **BERTopic model for German**\n",
    "\n",
    "We repeated the same setup for the German dataset; however, instead of using the Amazon multilingual review dataset, we used  the Amazon MASSIVE intent dataset_german and followed the same procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b50f2-e841-453e-975c-59eeeda9c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model_de = KeyBERTInspired()\n",
    "\n",
    "topic_model_baseline_de = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"german\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    min_topic_size=30,\n",
    "    nr_topics=None,\n",
    "    representation_model=representation_model_de\n",
    ")\n",
    "\n",
    "topics_de, probs_de = topic_model_baseline_de.fit_transform(docs_train_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d403ba-7153-4c9c-b6b5-c0f92a39a023",
   "metadata": {},
   "source": [
    "##### **BERTopic model for English**\n",
    "\n",
    "We repeat the same setup for the English Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23412a6-2d2b-4fa4-8ec3-76e09ad9be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model_en = KeyBERTInspired()\n",
    "\n",
    "topic_model_baseline_en = BERTopic(\n",
    "    embedding_model=sentence_model,      \n",
    "    vectorizer_model=vectorizer_model,   \n",
    "    language=\"english\",                  \n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    min_topic_size=30,\n",
    "    nr_topics=None,                      \n",
    "    representation_model=representation_model_en\n",
    ")\n",
    "\n",
    "topics_en, probs_en = topic_model_baseline_en.fit_transform(docs_train_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11d7b44-cd48-466b-b7d1-ecb95923ff90",
   "metadata": {},
   "source": [
    "##### **BERTopic model for Spanish**\n",
    "\n",
    "We repeat the same setup for the Spanish Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76748850-dabf-4fe0-8905-ba75c387798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model_spanish = KeyBERTInspired()\n",
    "\n",
    "topic_model_baseline_spanish = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"spanish\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    min_topic_size=30,\n",
    "    nr_topics=None,\n",
    "    representation_model=representation_model_spanish\n",
    ")\n",
    "\n",
    "topics_base_spanish, probs_base_spanish = topic_model_baseline_spanish.fit_transform(docs_train_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a0675-0540-49e8-9553-8794a8970e1c",
   "metadata": {},
   "source": [
    "### **Inspecting computed topics for Italian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebe59e-f9ec-4d08-96f2-62488adf2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = topic_model_baseline_italian.get_topic_info()\n",
    "clean_df = info_df[info_df[\"Topic\"] != -1].copy()\n",
    "\n",
    "def clean_keywords(repr_list):\n",
    "    return \", \".join(repr_list[:6])\n",
    "\n",
    "clean_df[\"Top Keywords\"] = clean_df[\"Representation\"].apply(clean_keywords)\n",
    "\n",
    "display_table = clean_df[[\"Topic\", \"Count\", \"Top Keywords\"]]\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(display_table.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfe43c-c06e-431a-9ae3-c7418f8a3055",
   "metadata": {},
   "source": [
    "Findings: The model is able to identify some meaningful intent-related themes in the Italian MASSIVE dataset. For example topic-1 showed somewhat clearer structure, capturing media-playback actions through verbs like **“mettere”** and **“metti”** (put/play) and nouns such as **“brani”** (songs). On the other hand, BERTopic in italian dataset produced several mixed or noisy clusters, indicating difficulty in extracting clean intent categories from short Italian utterances. However, the output also highlights key limitations. For instance, Topic 0 was dominated by extremely frequent conversational tokens such as **“sono”** (I am), **“favore”** (please), and even **“chiocciola”** (the @ symbol).\n",
    "\n",
    "Together, these examples show that BERTopic struggles to form clean intent clusters when input texts are short, formulaic, and dominated by high-frequency filler words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22f98f-586b-44a6-8277-5c7f44645e3b",
   "metadata": {},
   "source": [
    "### **Inspecting computed topics for English**\n",
    "\n",
    "We repeat the same steps for the inspection of the topics generated by the French model by building a clean topic overview table with interpretable keyword representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c03ba6-7980-440a-9144-dfc3e9d8c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = topic_model_baseline_en.get_topic_info()\n",
    "clean_df = info_df[info_df[\"Topic\"] != -1].copy()\n",
    "\n",
    "def clean_keywords(repr_list):\n",
    "    return \", \".join(repr_list[:6])\n",
    "\n",
    "clean_df[\"Top Keywords\"] = clean_df[\"Representation\"].apply(clean_keywords)\n",
    "\n",
    "display_table = clean_df[[\"Topic\", \"Count\", \"Top Keywords\"]]\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(display_table.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2da017-5ba6-4c36-8633-1757d8e43e69",
   "metadata": {},
   "source": [
    "As predicted, the English BERTopic model performs noticeably better than the Italian and German versions, producing cleaner and more semantically consistent clusters. For example, Topic 0 captures a coherent lighting-control intent with keywords such as **“darken,”** **“lighting,”** **“lamp,”** **“brighten,”** and **“lights”**, which aligns well with typical smart-home commands.\n",
    "\n",
    "However, some clusters still suffer from noise and mixed semantics. Topic 5, for instance, blends celebrity names **(“kim,” “elvis,” “miley”) **with unrelated temporal words **(“when”)**, indicating that BERTopic occasionally forms clusters based on accidental lexical co-occurrence rather than true intent similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6a23-8408-4f8f-9fd5-ab8c55564461",
   "metadata": {},
   "source": [
    "### **Inspecting computed topics for German**\n",
    "\n",
    "We repeat the same steps for the inspection of the topics generated by the French model by building a clean topic overview table with interpretable keyword representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cc7bf-9ee1-4575-b9f8-248e8fb5d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = topic_model_baseline_de.get_topic_info()\n",
    "clean_df = info_df[info_df[\"Topic\"] != -1].copy()\n",
    "\n",
    "def clean_keywords(repr_list):\n",
    "    return \", \".join(repr_list[:6])\n",
    "\n",
    "clean_df[\"Top Keywords\"] = clean_df[\"Representation\"].apply(clean_keywords)\n",
    "\n",
    "display_table = clean_df[[\"Topic\", \"Count\", \"Top Keywords\"]]\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(display_table.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467b13f-a613-4fa5-a331-347d6e5eaa7d",
   "metadata": {},
   "source": [
    "Findings: The German BERTopic model shows mixed performance.\n",
    "On the positive side, one cluster cleanly captures email-related actions, with keywords like in topic 2 **“posteingang”** (inbox), **“öffne” **(open), **“erhalten”** (receive), and **“schicke”** (send), indicating that the model successfully extracts a coherent communication-intent topic.\n",
    "\n",
    "\n",
    "However, a clear limitation appears with the repeated use of the politeness word “bitte.” It shows up in Topic 0, Topic 3, Topic 5, and Topic 6, even though these topics correspond to different intents such as media playback, device control, daily schedule etc. Because\n",
    "“bitte”is so common in short German commands, BERTopic overweights it, which causes clusters to mix polite filler words with the actual intent-related verbs and it reduced the overall clarity of the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381edd0-63a8-4f95-ac04-eeb44b3fa2b3",
   "metadata": {},
   "source": [
    "### **Inspecting computed topics for Spanish**\n",
    "\n",
    "We repeat the same steps for the inspection of the topics generated by the Spanish model by building a clean topic overview table with interpretable keyword representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a670613-e5d4-4bc1-80d1-bbc6289b6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = topic_model_baseline_spanish.get_topic_info()\n",
    "clean_df = info_df[info_df[\"Topic\"] != -1].copy()\n",
    "\n",
    "def clean_keywords(repr_list):\n",
    "    return \", \".join(repr_list[:6])\n",
    "\n",
    "clean_df[\"Top Keywords\"] = clean_df[\"Representation\"].apply(clean_keywords)\n",
    "\n",
    "display_table = clean_df[[\"Topic\", \"Count\", \"Top Keywords\"]]\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(display_table.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34bcab-7e5f-4a77-8253-4daa9442f11a",
   "metadata": {},
   "source": [
    "Findings: Like other language some topics capturing real assistant style intents. For example, Topic 3 groups together home related commands with keywords like “casa” (house), “salón” (living room), “porche” (porch), and “apagar” (turn off), indicating a coherent cluster focused on smart home control. However, several limitations also appear. Topic 0 contains extremely general words like “ver” (see), “algún” (some), and “favor” (please), which do not correspond to a specific intent. High-frequency filler terms such as “quiero” (I want) and “para” (for) show up across multiple clusters, reducing their distinctiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc842e9-a12e-4625-9a26-6d8a3289c7b4",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0dde09-1aa9-41c9-9069-3ee2b93253e6",
   "metadata": {},
   "source": [
    "#### **Visualization of Italian topic space**\n",
    "\n",
    "Similar to the previous notebook, we visualize the topics in a 2D projection to assess how well-separated they are.We did in three different language italian, english, spanish german for amazon massive intent dataset. The accompanying bar chart allows us to inspect the key words within each cluster along with their relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986745e7-ce01-4cbe-93fe-ba011a4de88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_map = topic_model_baseline_italian.visualize_topics()\n",
    "fig_map.show()\n",
    "\n",
    "fig_bar = topic_model_baseline_italian.visualize_barchart(top_n_topics=10)\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160b525-e7cc-4b87-9e74-c8d5e67c2a12",
   "metadata": {},
   "source": [
    "#### **Visualization of the English topic space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42eda63-feca-488d-8477-d504f56512c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_map = topic_model_baseline_en.visualize_topics()\n",
    "fig_map.show()\n",
    "\n",
    "fig_bar = topic_model_baseline_en.visualize_barchart(top_n_topics=10)\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d2607-6423-4eb0-b571-29a7e785edfa",
   "metadata": {},
   "source": [
    "#### **Visualization of the German topic space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6cab04-e578-4a93-98f3-b2f1f7f9bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_map = topic_model_baseline_de.visualize_topics()\n",
    "fig_map.show()\n",
    "\n",
    "fig_bar = topic_model_baseline_de.visualize_barchart(top_n_topics=10)\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8cc3c-ebbd-47b0-b837-71dc59a9405c",
   "metadata": {},
   "source": [
    "#### **Visualization of the Spanish topic space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f795f-0490-4d44-8354-ea5e95776217",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_map = topic_model_baseline_spanish.visualize_topics()\n",
    "fig_map.show()\n",
    "\n",
    "fig_bar = topic_model_baseline_spanish.visualize_barchart(top_n_topics=10)\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f236b2-c557-437f-82b0-b22977faebdd",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04443606-6070-4a14-bead-a26a74190f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "from math import log\n",
    "\n",
    "class TopicModelWrapper:\n",
    "    def __init__(self, topics):\n",
    "        self.topics = topics\n",
    "\n",
    "    def get_topics(self):\n",
    "        return self.topics\n",
    "\n",
    "def evaluate_bertopic_pmi(\n",
    "    topic_model,\n",
    "    docs,\n",
    "    top_k_coherence: int = 5,\n",
    "    top_k_diversity: int = 10,\n",
    "    skip_outlier: bool = True,\n",
    "    tag: str = \"Baseline\",\n",
    "):\n",
    "\n",
    "    \n",
    "    raw_topics = topic_model.get_topics()\n",
    "\n",
    "    new_keywords = {\n",
    "        topic_id: [word for word, _ in word_scores]\n",
    "        for topic_id, word_scores in raw_topics.items()\n",
    "    }\n",
    "\n",
    "    if skip_outlier and -1 in new_keywords:\n",
    "        new_keywords = {tid: kws for tid, kws in new_keywords.items() if tid != -1}\n",
    "\n",
    "\n",
    "    all_keywords = sorted({w for kws in new_keywords.values() for w in kws})\n",
    "\n",
    "    vectorizer = CountVectorizer(vocabulary=all_keywords, lowercase=True)\n",
    "    X = vectorizer.fit_transform(docs)  # shape: (n_docs, n_terms)\n",
    "\n",
    "    n_docs, n_terms = X.shape\n",
    "\n",
    "    X_bin = (X > 0).astype(int)\n",
    "\n",
    "    word_doc_counts = np.asarray(X_bin.sum(axis=0)).ravel()\n",
    "    p_w = word_doc_counts / n_docs\n",
    "\n",
    "    cooc_counts = (X_bin.T @ X_bin).toarray()\n",
    "    p_ij = cooc_counts / n_docs\n",
    "\n",
    "    vocab = np.array(vectorizer.get_feature_names_out())\n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "\n",
    "    def npmi_pair(w1, w2):\n",
    "        i = word2id.get(w1)\n",
    "        j = word2id.get(w2)\n",
    "        if i is None or j is None:\n",
    "            return None\n",
    "\n",
    "        pij = p_ij[i, j]\n",
    "        if pij == 0:\n",
    "            return None  # never co-occur\n",
    "\n",
    "        pi = p_w[i]\n",
    "        pj = p_w[j]\n",
    "\n",
    "        pmi = log(pij / (pi * pj))\n",
    "        return pmi / (-log(pij))\n",
    "\n",
    "    def topic_npmi_coherence(topic_words, top_k=None):\n",
    "        if top_k is not None:\n",
    "            topic_words = topic_words[:top_k]\n",
    "\n",
    "        scores = []\n",
    "        for w1, w2 in itertools.combinations(topic_words, 2):\n",
    "            score = npmi_pair(w1, w2)\n",
    "            if score is not None:\n",
    "                scores.append(score)\n",
    "\n",
    "        if not scores:\n",
    "            return float(\"nan\")\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    topic_scores = {\n",
    "        topic_id: topic_npmi_coherence(words, top_k=top_k_coherence)\n",
    "        for topic_id, words in new_keywords.items()\n",
    "    }\n",
    "\n",
    "    coherence_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Topic\": list(topic_scores.keys()),\n",
    "            \"NPMI\": list(topic_scores.values()),\n",
    "        }\n",
    "    ).sort_values(\"Topic\")\n",
    "\n",
    "    mean_npmi = float(np.nanmean(coherence_df[\"NPMI\"]))\n",
    "\n",
    "\n",
    "    def topic_diversity(topics_dict, k=10):\n",
    "        # collect top-k words for each topic\n",
    "        topk_words = []\n",
    "        for tid, words in topics_dict.items():\n",
    "            topk_words.extend(words[:k])\n",
    "\n",
    "        if not topk_words:\n",
    "            return float(\"nan\")\n",
    "\n",
    "        unique_words = set(topk_words)\n",
    "        T = len(topics_dict)\n",
    "        total_words = T * k\n",
    "\n",
    "        return len(unique_words) / total_words\n",
    "\n",
    "    diversity = float(topic_diversity(new_keywords, k=top_k_diversity))\n",
    "\n",
    "    print(f\"{tag} Model - NPMI: {mean_npmi:.4f}\")\n",
    "    print(f\"{tag} Model - Diversity: {diversity:.4f}\")\n",
    "\n",
    "    return coherence_df, mean_npmi, diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0f0e1-770e-4909-9fa7-b9fb737f805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_, npm_german, div_german = evaluate_bertopic_pmi(\n",
    "    topic_model=topic_model_baseline_de,\n",
    "    docs=docs_cleaned_de,\n",
    "    top_k_coherence=10,\n",
    "    top_k_diversity=25,\n",
    "    tag=\"German\"\n",
    ")\n",
    "\n",
    "_, npm_italian, div_italian = evaluate_bertopic_pmi(\n",
    "    topic_model=topic_model_baseline_italian,\n",
    "    docs=docs_cleaned_italian,\n",
    "    top_k_coherence=10,\n",
    "    top_k_diversity=25,\n",
    "    tag=\"Italian\"\n",
    ")\n",
    "_, npm_en, div_en = evaluate_bertopic_pmi(\n",
    "    topic_model=topic_model_baseline_en,\n",
    "    docs=docs_cleaned_english,\n",
    "    top_k_coherence=10,\n",
    "    top_k_diversity=25,\n",
    "    tag=\"English\"\n",
    ")\n",
    "_, npm_es, div_es = evaluate_bertopic_pmi(\n",
    "    topic_model=topic_model_baseline_spanish,\n",
    "    docs=docs_cleaned_es,\n",
    "    top_k_coherence=10,\n",
    "    top_k_diversity=25,\n",
    "    tag=\"Spanish\"\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Coherence (NPMI)\", \"Diversity\"],\n",
    "    \"German\": [npm_german, div_german],\n",
    "    \"Italian\": [npm_italian, div_italian],\n",
    "    \"English\": [npm_en, div_en],\n",
    "    \"Spanish\": [npm_es, div_es]\n",
    "})\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c5553-7aac-4176-b48a-8d2d5ad27c35",
   "metadata": {},
   "source": [
    "# Overall Summary:\n",
    "\n",
    "The goal of this project was to replicate the success observed in our English news experiment and, to some extent, in the more challenging multilingual Amazon Reviews dataset, **using a simpler dataset to evaluate whether the model would perform better.** Additionally, we sought to compare how the Italian and spanish language behaves relative to the others. Previously, we experimented with the German Amazon multilingual review dataset, but the data proved too noisy due to its nature as a review corpus.\n",
    "\n",
    "Although we were able to extract intent-related themes for Italian, Spanish, German, and English, the quantitative metrics indicate that the results were again considerably less successful. The coherence scores for German, Italian and spanish were 0.29 and 0.18, 0.19 respectively. We expected the English portion of the Amazon MASSIVE Intent dataset to perform better; however, it produced a coherence score of 0.19, which is also very low. In contrast, the BBC News dataset achieved a coherence score of 0.40, highlighting the performance gap. Although the coherence score for the German dataset increased slightly from 0.26 to 0.29, this improvement is minimal.\n",
    "\n",
    "\n",
    "\n",
    "With the Amazon Reviews dataset, we initially hypothesized that the model underperformed due to the nature of review data—texts that are short, informal, and heavily saturated with sentiment (e.g., “bad,” “perfect”), which complicates topic extraction. However, the Amazon MASSIVE Intent dataset is simple and clean, without slang, typos, or strong sentiment. Despite this, the model still performed poorly.\n",
    "\n",
    "These results suggest that the multilingual BERTopic model may still have notable limitations, particularly when applied to intent-based datasets across different languages.On the other hand, the availability of suitable, high-quality datasets in multiple languages is limited, which further constrains our ability to comprehensively evaluate BERTopic’s multilingual performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepfake)",
   "language": "python",
   "name": "deepfake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
